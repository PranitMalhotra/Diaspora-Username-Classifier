{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Diaspora Username Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// Have to change this according to Diaspora and stuff\n",
    "This machine learning project aims to discover the feasibility of classifying solely based on the usernames. \n",
    "\n",
    "Sklearn's Logistic Regression, Random Forest and SVM are chosen as the baseline models for this binary classification project.<br>\n",
    "Results would be either \"True\" for HK users or \"False\" for non-HK users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import SyllableTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42 # set the random seed to 42 for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is scraped from [HypeAuditor](https://hypeauditor.com/) using a web scraping script (**scraper.py**).\n",
    "\n",
    "The script scrapes the top 1000 most popular Instagram Users from:\n",
    "1. Indian Usernames (Name of the Indian Dataset)\n",
    "2. Diaspora - Sensitive COuntry Usernames (Name of the Dataset file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraper import HypeAuditorScraper\n",
    "\n",
    "scraper = HypeAuditorScraper()\n",
    "indian_username_df = scraper.scrape(os.getenv(\"gmail\"), os.getenv(\"password\"), india = True)\n",
    "diaspora_sensitive_username_df = scraper.scrape(os.getenv(\"gmail\"), os.getenv(\"password\"), india = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_folder_path = os.path.join(os.getcwd(), 'datasets')\n",
    "os.makedirs(datasets_folder_path)\n",
    "os.chdir(datasets_folder_path)\n",
    "indian_username_df.to_csv(\"indian_username.csv\")\n",
    "diaspora_sensitive_username_df.to_csv(\"diaspora_sensitive_username.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Merging Datasets and Labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the two datasets **indian_username.csv** and **diaspora_sensitive_username.csv** into a dataframe and label Indian users as True and Diaspora-Sensitive users as False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_folder_path = os.path.join(os.getcwd(), 'datasets')\n",
    "os.chdir(datasets_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indian_username_df = pd.read_csv(\"indian_username.csv\")\n",
    "indian_username_df[\"Indian\"] = True\n",
    "diaspora_sensitive_username_df = pd.read_csv(\"diaspora_sensitive_username.csv\")\n",
    "diaspora_sensitive_username_df[\"Indian\"] = False\n",
    "\n",
    "df = pd.concat([indian_username_df, diaspora_sensitive_username_df], axis = 0, ignore_index = True) #have to understand axis and ignore_index\n",
    "df = df.drop(df.columns[0], axis = 1) # why are dropping this dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop duplicated entries** to prevent skewed distribution.\n",
    "\n",
    "Can maybe add the insight that how many of the top accounts are the same in both of the data frame. Need to see if that is a fault of poor webscrapping i.e. poor choice of source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_size = len(df['IG Username'])\n",
    "df = df.drop_duplicates(subset=['IG Username']) \n",
    "new_size1 = len(df['IG Username'])\n",
    "print(f'{original_size - new_size1} duplicated entries are removed, {new_size1} entries are retained.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removal of numbers and punctuations**. Since numbers and punctuations do not contain userful information in classifying, RegEx is used to remove them. <br> Then, drop empty entries because some usernames may only contain numbers and punctuations.\n",
    "\n",
    "\n",
    "For example:\n",
    "```\n",
    "\"emi_martinez26\" -> \"emimartinez\"\n",
    "\"505\" -> \"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"IG Username\"] = df[\"IG Username\"].apply(lambda x: re.sub(r'[\\d._]+', '', x))\n",
    "df = df[df['IG Username'] != \"\"]\n",
    "new_size2 = len(df['IG Username'])\n",
    "print(f'{new_size1 - new_size2} empty entries are removed, {new_size2} entries are retained.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are 4 reasons to tokenize usernames based on syllables:\n",
    "\n",
    "1. **No Whitespaces between Words** \n",
    "    * Lack of whitespaces in the Instagram usernames makes the traditional tokenizers that heavily rely on whitespaces, less effective.\n",
    "\n",
    "2. **Usernames are not Sentences**\n",
    "    * In other words, usernames are too short to extract a \"word\" as a unit for the features.\n",
    "\n",
    "3. **Usernames are not composed of English Vocabulary**\n",
    "    * Usernames are not always composed of English vocabulary, thus any conventional tokenizers will not have the word embeddings for usernames. Even a subword tokenizer that tokenizes a word based on the prefixes and suffixes would not work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using NLTK syllable tokenizer to tokenize the usernames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_model = SyllableTokenizer()\n",
    "df[\"Tokenized IG Username\"] = df[\"IG Username\"].apply(lambda x: np.array(tokenizer_model.tokenize(x))) \n",
    "df[\"Tokenized IG Username\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert categorical features (tokenized usernames) into binary data for easier model interpretation using one-hot encoding (OHE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_syllables = df[\"Tokenized IG Username\"].explode().unique()\n",
    "for i in unique_syllables:\n",
    "  df[i] = df[\"Tokenized IG Username\"].apply(lambda syllable_list: int(i in syllable_list))\n",
    "X = df.iloc[:,3:]\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Distribution of Indian and non-Indian Entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During data collection, the dataset is balanced as there are equally 1000 Indian and 1000 non-Indian entries. However, some of the entries are dropped after data cleaning. Therefore, the distribution of data needs to be checked again to avoid biased model performance.\n",
    "\n",
    "In this case, Indian comprise 49.4% **(NEED TO CHANGE)** and non-Indian comprises 50.6% **(NEED TO CHANGE)** of the entries. So, further data processing is not required as <u>data is not biased</u>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(ind):\n",
    "  return len(df.loc[(df['Indian'] == ind), 'Indian'])\n",
    "\n",
    "plt.pie([getData(1), getData(0)], \n",
    "        labels = ['Indian', 'Non - Indian'], autopct = '%1.1f%%')\n",
    "plt.title(\"Distribution of Indian and Non-Indian Entries\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Distribution of Repeated and Unique Syllables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Have to rewrite this completely.**\n",
    "\n",
    "To give an overview of 4630 (65.6%) syllables are repeated and can be used as useful features in classifying. <br> This is because reapeated syllables can act as a pattern for the classifying model to **generalize** the usernames rather than to memorize the unique syllables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_syllables = len(df[\"Tokenized IG Username\"].explode())\n",
    "unique_syllables = len(df[\"Tokenized IG Username\"].explode().unique())\n",
    "repeated_syllables = all_syllables - unique_syllables\n",
    "\n",
    "print(f'There are {repeated_syllables} repeated syllables and {unique_syllables} unique syllables.')\n",
    "\n",
    "plt.pie([repeated_syllables, unique_syllables], \n",
    "        labels = ['Repeated Syllables', 'Unique Syllables'], autopct = '%1.1f%%')\n",
    "plt.title(\"Distribution of Repeated and Unique Syllables\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following pie charts are used to further investigate the distribution of repeated and unique syllables in Indian and non-Indian users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(ind):\n",
    "  return df.loc[~(df['Indian'] == ind), 'Tokenized IG Username']\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3))\n",
    "\n",
    "for i in range(2):\n",
    "  total_syllables = len(getData(i).explode())\n",
    "  unique_syllables = len(getData(i).explode().unique())\n",
    "  repeated_syllables = total_syllables - unique_syllables\n",
    "\n",
    "  title = \"Indian\" if i == 0 else \"non-Indian\"\n",
    "  ax[i].pie([repeated_syllables, unique_syllables], \n",
    "          labels = ['Repeated Syllables', 'Unique Syllables'], autopct = '%1.1f%%')\n",
    "  ax[i].set_title(f\"Distribution of Repeated \\n and Unique Syllables in {title} Users\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Have to change this according to the results that I get**\n",
    "\n",
    "From above, we can see that (55.0% repeated, 45% unique) for HK users and (63.3% repeated, 36.7% unique) for Non-HK users. <br> Overall, Non-HK users have a higher percentage of repeated syllables. *The classifying models might result in classifying the Non-HK usernames better as there are more repeated syllables available.*\n",
    "\n",
    "To conclude, the percentage of repeated and unique syllables are quiet reasonable as there are multiple syllables in the username and the classifying model might fail to generalize the pattern of the username if there are too much repeated syllables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Visualizing the Potential Patterns between Syllables using t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used an unsupervised dimensionality reduction technique t-SNE (t-Distributed Stochastic Neighbor Embedding) to give an initial overview of the possible underlying patterns of syllables in the usernames. \n",
    "\n",
    "In this case, dim(2425) is reduced to dim(2). **Have no idea what this is**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components = 2, random_state = SEED) #Have to change this in cell 1 as well and see what it is\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "df_tsne = pd.DataFrame(data = X_tsne, columns = ['tsne_x', 'tsne_y'])\n",
    "df_tsne[\"target\"] = df['Indian']\n",
    "\n",
    "for i in range(2):\n",
    "  color = ['tab:blue', 'tab:orange']\n",
    "  plt.scatter(x = df_tsne.loc[(df_tsne['target'] == i), 'tsne_x'], \n",
    "              y = df_tsne.loc[(df_tsne['target'] == i), 'tsne_y'], \n",
    "              color = color[i], alpha = 0.5)\n",
    "\n",
    "plt.title(\"t-SNE Visualization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Have to change this according to the graph that I get**\n",
    "\n",
    "From the graph above, no useful insights can be observed as the scatters do not form any distinct clusters.\n",
    "\n",
    "A possible factor to this might be each syllable in the usernames is treated independently (per column) but not the entire username (per row). <br> In addition, curse of dimensionality could also be the reason as reducing dimensionality from 2425 to just 2 can lose a lot of information considering the dataset is small and each feature might only have a few entries. Therefore, the resulting t-SNE plot may not reveal distinct clusters or patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Further Analysis on the Syllables using Linguistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highlighting some of the more interesing **naming patterns** and **behaviors** of the Syllable Tokenizer for both Indian and non-Indian entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(ind):\n",
    "  return df.loc[~(df[\"Indian\"] == ind), \"Tokenized IG Username\"].explode().value_counts() # already descending\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (15, 5))\n",
    "\n",
    "for i in range(2):\n",
    "  top_ten_freq = getData(i).head(10).sort_index().sort_index(key = lambda x: x.str.len()) # sort alphabetically then sort by length\n",
    "  x = top_ten_freq.index\n",
    "  y = top_ten_freq.values\n",
    "\n",
    "  title = \"Indian\" if i == 0 else \"non-Indian\"\n",
    "  sns.barplot(x = x, y = y, ax = ax[i])\n",
    "  ax[i].set_title(f\"Top 10 Most Appearing Syllables in {title} Usernames\") # Need to use termcolor here\n",
    "  ax[i].set_ylim(0, 80)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HAS TO BE COMPLETELY CHANGED**\n",
    "\n",
    "Terminologies:\n",
    "\n",
    "* **Vowels**: a,e,i,o,u,y* and can be a standalone syllable\n",
    "* **Consonants**: characters that are not vowels and cannot be a standalsone syllable\n",
    "* **Consonant-vowel (CV) syllables**: a syllable that contain both vowel and consonants, e.g. 'fi', 'ha', etc.\n",
    "* **Monosyllabic**: single/ one syllable\n",
    "\n",
    "*Note: y sometimes can act as a vowel as well*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HAS TO BE COMPLETELY CHANGED**\n",
    "\n",
    "#### Explanation of the Results\n",
    "\n",
    "  1. **Higher Appearance of standalone Vowels in HK** -> Unique Vowel Clusters\n",
    "      * Romanized Cantonese has a lot of unique adjacent vowels compared to English or other languages\n",
    "      * for example: {\"張\": \"ch-***eu***-ng\", \"楊\": \"  ***yeu***-ng\", \"趙\": \"ch-***iu***\", \"游\": \"  ***yau***\", ...}\n",
    "      * the tokenizer is not familiar with these clusters and might treat them as an individual syllable\n",
    "\n",
    "  2.  **Less CV syllables, more Unique Syllables in HK**-> Complex Consonants Clusters\n",
    "      * Romanized Cantonese also has a lot of complex consonants combinations and some can even contain no vowels at all.\n",
    "      * for example : {\"翠\": \"***ts***-ui\", \"芷\": \"  ***tsz***\", \"吳\": \"***ng***\", \"郭\": \"***kw***-ok\", ...}\n",
    "      * This confuses the tokenizer to group the consonants to other vowels, resulting in more unique syllables\n",
    "\n",
    "  3. **Lower Frequency of a syllable in HK** -> Monosyllabic Chinese Characters\n",
    "      * the maximum frequency of a syllable is around 75 (\"ha\") in Non-HK while it is only roughly 50 (\"i\") in HK \n",
    "      * Hong Kong People's name are mostly made up of 3 Chinese characters, and each chinese character only has one syllable\n",
    "      * i.e. Hong Kong People's name at most have 3 syllables and leads to overall lower frequency of syllables in usernames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HAS TO BE COMPLETELY CHANGED**\n",
    "\n",
    "However, it is crucial to understand that usernames are a complicated subject and do not only contain user's actual name. For instance, the high presence of the \"**-cial**\" syllable in non-HK usernames is an evidence of individuals incorporating elements other than their name, such as the word \"official,\" into their usernames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting the **Feature Matrix** (x) and **Target Vector** (y) into training dataset (**80%**) and testing dataset (**20%**).\n",
    "\n",
    "Splitting into two sets can help prevent overfitting by ensuring that the model does not become overly tailored to the training data that has not been split, thus enabling better generalization to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,3:]\n",
    "y = df[\"Indian\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the size\n",
    "data_name_list = [\"X_train\", \"y_train\", \"X_test\", \"y_test\"]\n",
    "arr = [X_train, y_train, X_test, y_test]\n",
    "for i in range(4):\n",
    "  print(f\"Size of {data_name_list[i]}: {arr[i].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once more, we examine the distribution of the dataset after it has been split to detect any **skewing**. \n",
    "\n",
    "The following graphs show that both training data and testing data are balanced.\n",
    "\n",
    "**HAVE TO CHANGE ACCORDING TO WHAT I GET**\n",
    "\n",
    "which are 49.3% HK, 50.6% Non-Hk and 49.3% HK, 50.7% Non-HK respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(train, ind):\n",
    "  return len(y_train[y_train == ind]) if train == 0 else len(y_test[y_test == ind])\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(7, 3))\n",
    "\n",
    "for i in range(2):\n",
    "    title = ['Training Dataset', 'Testing Dataset']\n",
    "    ax[i].pie([getData(i,1), getData(i,0)], labels = ['Indian', 'non-Indian'], autopct='%1.1f%%')\n",
    "    ax[i].set_title(f\"Distribution of Indian and non-Indian entries \\n in {title[i]}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Choosing Models for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Need to fact check all this**\n",
    "**Can improve this my a 100 times**\n",
    "\n",
    "#### Brief Explanation of the Models Chosen:\n",
    "\n",
    "1. **Logistic Regression (LR)**\n",
    "    * Assumes that the data follows a Bernoulli distribution (0 or 1)\n",
    "    * Maximizes the likelihood of the data with a gradient descent \n",
    "    * Calculates the probability using the logistic function\n",
    "\n",
    "2. **Random Forest (RF)**\n",
    "    * two types of node in a binary decision tree: \n",
    "        * conditional node (set conditions and branch one more pair of condtional and leaf nodes) \n",
    "        * leaf node (predicted values/ data that satisfy all the conditions)\n",
    "    * maximizes the separation of classes by choosing one of the many decision trees (ensemble)\n",
    "    * create mulitple trees because the conditions set at the root node are arbitrary and can vastly affect the results \n",
    "     \n",
    "3. **Supported Vector Machines (SVM)**\n",
    "    * treat the data as vectors in a vector space that is higher than original dimensionality of the data (kernal trick)\n",
    "    * maximizes the margin between classes by finding the optimal hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of the Decision Boundaries\n",
    "\n",
    "**REWRITE CHATGPT**\n",
    "\n",
    "Though this visualization only gives a partial overview of the decision boundaries, as ```X_test``` is being reduced to 2D with Principal Component Analysis (PCA), it is good enough to show a general idea of how the decision boundaries are formed by different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(X_train, y_train):\n",
    "  \n",
    "  models = [('LR', LogisticRegression()),\n",
    "            ('RF', RandomForestClassifier(random_state=SEED)),\n",
    "            ('SVM', SVC())]\n",
    "\n",
    "  fig, ax = plt.subplots(1, 3, figsize = (15, 5)) \n",
    "\n",
    "  pca = PCA(n_components = 2)\n",
    "  re_X_train = pca.fit_transform(X_train)\n",
    " \n",
    "  for i in range(3):    \n",
    "    clf = models[i][1]\n",
    "    clf.fit(re_X_train, y_train)\n",
    "    plot_decision_regions(X = re_X_train, y = y_train.to_numpy().astype(int), clf = clf, legend = 2, ax = ax[i])\n",
    "    \n",
    "    name = models[i][0]\n",
    "    ax[i].set_title(f'Decision Boundary of {name}')\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "visualize(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanation of the Visualization:\n",
    "\n",
    "**REWRITE CHATGPT**\n",
    "\n",
    "1. **Logistic Regression (LR)** \n",
    "    * the division boundary is linear \n",
    "    * might not be able to capture non-linear relationships\n",
    "\n",
    "2. **Random Forest (RF)**\n",
    "    * the division boundary is non-linear \n",
    "    * be able to capture very complex non-linear relationships \n",
    "    * could be a double-edged sword as it can easily underfit or overfit \n",
    "     \n",
    "3. **Supported Vector Machines (SVM)**\n",
    "    * the division boundary is non-linear\n",
    "    * be able to capture non-linear relationships\n",
    "    * effective in handling high-dimensional features\n",
    "    * very sensitive to the choice of kernel and its hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REWRITE CHATGPT**\n",
    "\n",
    "Overall, SVM has the potential to be the best choice if all the hyperparameters are properly tuned, but since the dataset is small, it is still essential to evaluate and compare the performance of all the models on this specific dataset. By tuning and deploying all three models, we can determine which one yields the best results and is most suitable for the given task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Executing Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**None of this makes any sense have to see what to do**\n",
    "\n",
    "An exhaustive search method in Sklearn ```GridSearchCV()``` that combines grid search and k-fold validation is used to tune the hyperparameters. <br> During this process, hyperparameter tuning, model training and 5-fold validation are performed simultaneously.\n",
    "\n",
    "All the testing hyperparameters are listed in the parameter grid ```param_grid``` and are determined based on accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brief Explanation on the Choice of Testing Hyperparameters\n",
    "\n",
    "Since the dataset is **small**, contains a **lot of features** (2425 syllables), and **each feature may only have a few entries**, the model might have a higher tendency to memorize the limited samples in the training data set and fail to generalize on unseen data.\n",
    "\n",
    "So the main priority here for tuning the hyperparameters would be to prevent overfitting. Common approaches would be regularizing the data in LR, determining the levells of conditions ran through in RF and find the optimal margin in SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hpTuning(X_train, y_train):\n",
    "\n",
    "    # three different classification models\n",
    "    clf = [LogisticRegression(), RandomForestClassifier(random_state = SEED), SVC()]\n",
    "\n",
    "    param_grid = [{\n",
    "        'solver': ['liblinear'], # good for small datasets and binary classification; supports l1 and l2 regularization\n",
    "        'penalty': ['l1', 'l2'], # 2 different regularization methods are used to avoid overfitting by adding a penalty term\n",
    "    'C': [1, 10, 20] # regularization strength; smaller c greater regularization strength\n",
    "    },\n",
    "    {\n",
    "    'n_estimators': [100, 150], # number of trees in the 'forest'\n",
    "    'max_depth': [3, 5, 7] # the data run through how many levels of conditions\n",
    "    },\n",
    "    {\n",
    "    'C': [1, 10, 20], # smaller c greater margin\n",
    "    'gamma': ['auto', 'scale'] # 1 / (n_features * X.var()), 1 / n_features\n",
    "    }]\n",
    "\n",
    "    best_clf = []\n",
    "\n",
    "    for i in tqdm(range(3)):\n",
    "    grid_search = GridSearchCV(estimator = clf[i], \n",
    "                                param_grid = param_grid[i], \n",
    "                                cv = 5,\n",
    "                                scoring = 'accuracy', \n",
    "                                n_jobs = -1) # -1 to enable parallel processing\n",
    "\n",
    "    grid_search.fit(X_train, y_train) # train\n",
    "    best_clf.append((grid_search.best_estimator_, \n",
    "                        grid_search.best_score_))   \n",
    "\n",
    "    return best_clf\n",
    "\n",
    "best_clf = hpTuning(X_train, y_train)\n",
    "results = pd.DataFrame(data = best_clf, \n",
    "                       index = ['LR', 'RF', 'SVM'],\n",
    "                       columns = [\"Hyperparameters\", \"Accuracy\"])\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning Results:\n",
    "\n",
    "1. **Logistic Regression (LR)**\n",
    "    * hyperparameter ```C``` is 1, which is relatively small and the regularization strength is strong\n",
    "    * it is not overfitting and has second best accuracy with **0.680702**\n",
    "\n",
    "2. **Random Forest (RF)**\n",
    "    * hyperparameter ```max_depth``` is 5, which indicates a moderately shallow tree and have limited number of conditions\n",
    "    * however, it has the worst accuracy with **0.649984**, so it might suggest that RF is underfitting\n",
    "\n",
    "3. **Support Vector Machines (SVM)**\n",
    "    * hyperparameter ```C``` is 1, which is relatively small and the margin between the support vectors is large.\n",
    "    * it is not overfitting and has best accuracy with **0.686049**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, SVM has the best performance while RF has the worst performance due to potential underfitting or overfiting. Nevertheless, the performance of LR is also noteworthy. Despite its linear division boundaries and potential limitations in capturing complex non-linear relationships, LR has shown competitive performance on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the testing performance of each model in a confusion matrix\n",
    "\n",
    "|             | Predicted Negative | Predicted Positive |\n",
    "|-------------|-------------------|-------------------|\n",
    "| Actual Negative |       True Negative (TN)          |       False Positive (FP)        |\n",
    "| Actual Positive |       False Negative (FN)          |       True Positive (TP)         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_test(best_clf, X_test, y_test):\n",
    "  \n",
    "  model_name = ['LR', 'RF', 'SVM']\n",
    "  ax = plt.subplots(1, 3, figsize = (15, 5))\n",
    "\n",
    "  for i in range(3):\n",
    "    y_pred = best_clf[i][0].fit(X_train, y_train).predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    " \n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = ['non-Indian', 'Indian'])\n",
    "    disp.plot(cmap = 'Blues', ax = ax[i])\n",
    "\n",
    "    accuracy = math.ceil(accuracy_score(y_test, y_pred)*1000)/1000 # round to 3 dp\n",
    "    ax[i].set_title(f'Confusion Matrix of {model_name[i]} \\n (Accuracy: {accuracy})')\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "performance_test(best_clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the results:\n",
    "\n",
    "Testing results have significantly better performance than validation results while RF is still the model with the worst performance (0.691).\n",
    "\n",
    "Surprisingly, both LR and SVM have identical results, achieving 0.742 accuracy, 133 TP, and 145 TN. <br> Moreover, with number of TN > TP, this also proves the hypothesis stated in *3.2*, 'The classifying models might result in classifying the Non-HK usernames better as there are more repeated syllables available.' is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Classification Report\n",
    "\n",
    "Showing other evaluation metrics like precision, recall, etc. in a tabular form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "model_name = ['LR', 'RF', 'SVM']\n",
    "for i in range(3):\n",
    "  y_pred = best_clf[i][0].fit(X_train, y_train).predict(X_test)\n",
    "  print(f'Classification Report of {model_name[i]}')\n",
    "  print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, by checking the f1-score (the harmonic mean of precision and recall) of LR or SVM, the f1-score of false (0.75) is slightly better than true (0.73). Both models are better at identifying Non-HK usernames than HK usernames. And all the metrics in RF are worse than RF or SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation and Conclusion\n",
    "### 7.1 Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To wrap up, I would choose **SVM** over LR for actual deployment due to its **scalability**. Even though they show a similar performance, as the dataset grows larger, LR may face challenges in capturing non-linear relationships of the syllables effectively. On the other hand, SVM is capable of handling non-linear relationships and can scale well to accommodate larger datasets with well-tuned hyperparameters and the kernel trick. However, if simplicity and computational cost are the main concerns, LR might be a more suitable choice to work with a small dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to ethics and Instagram's API policies, the size and quality of the collected usernames are restricted. This can vastly affect the model's performance and here are some limitations to the classification model due to the quality of training datasets:\n",
    "\n",
    "1. **Public Accounts**\n",
    "    * The dataset consists of the top 1000 most popular accounts in each country/region, which primarily include celebrities and influencers. These individuals are more likely to include their actual names in their usernames for recognition purposes. In contrast, regular users may prioritize privacy and may not include their real names in their usernames. This discrepancy can affect the model's performance, as it relies on the assumption that usernames contain unique Romanized Cantonese syllables. \n",
    "\n",
    "\n",
    "2. **English Names of HK Users**\n",
    "    * Due to historical reasons, many HK people have adopted English names, which they may include in their usernames. This poses a challenge for the syllable-based classification, as the uniqueness of Romanized Cantonese syllables is lost when English names are present. In this particular classification, Indian names were used as the non-HK dataset because they generally have more syllables, making them easier to distinguish. Therefore, the model's performance may not be as effective when applied to usernames from other English-speaking countries, such as the US or the UK. Nonetheless, it is not entirely impossible for the model to identify the unique English names preferred by HK users, such as \"Candy\" or \"Apple\", especially with a larger dataset.\n",
    "\n",
    "3. **Variability in Romanized Chinese**\n",
    "    * Besides Cantonese, there are other Chinese dialects and languages, such as Mandarin, that have their own Romanization systems. The variability in Romanized Chinese presents a challenge in accurately identifying and classifying usernames written in these different systems. This further adds to the complexity of the classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further enhance the performance of this model, the following several strategies can be implemented:\n",
    "\n",
    "1. **Larger Dataset** \n",
    "    * provide more diverse examples\n",
    "    * contains usernames from more different countries/ regions\n",
    "\n",
    "2. **Cantonese-specific Tokenizer**\n",
    "    * custom Romanized Cantonese embeddings\n",
    "    * to tokenize the usernames more accurately and consistently\n",
    "  \n",
    "3. **Additional Information**\n",
    "    * incorporating with users' Instagram bio \n",
    "    * bio can contain Cantonese characters, and if so, it will be more likely to be a HK user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, this project demonstrates the classification of Hong Kong Instagram usernames, in which the **SVM** model is achieving a **74.2% accuracy**, using an unconventional tokenization technique based on syllables. The core principle of this classification task revolves around the behavior of the NLTK syllable tokenizer and the distinctive linguistic features of Romanized Cantonese.\n",
    "\n",
    "Nevertheless, identifying usernames is a challenging topic and it is still important to acknowledge the limitations of this classification approach, such as the presence of public accounts, the inclusion of English names in HK users' usernames, and the variability in Romanized Chinese. Moreover, to enhance the model's performance, consider expanding the dataset, developing a Cantonese-specific tokenizer, and incorporating users' Instagram bios for improved classification results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
